{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e03b50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f029cd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_read(url):\n",
    "    local_file = url.split('/')[-1]\n",
    "    local_file = local_file.replace(\"%20\", \" \")\n",
    "    p = tf.keras.utils.get_file(local_file, url, extract = True, cache_dir = \".\")\n",
    "    local_folder = os.path.join(\"datasets\", local_file.split('.')[0])\n",
    "    labeled_sentences = []\n",
    "    for labeled_filename in os.listdir(local_folder):\n",
    "        if labeled_filename.endswith(\"_labelled.txt\"):\n",
    "            with open(os.path.join(local_folder, labeled_filename), \"r\") as f:\n",
    "                for line in f:\n",
    "                    sentence, label = line.strip().split(\"\\t\")\n",
    "                    labeled_sentences.append((sentence, label))\n",
    "    return labeled_sentences\n",
    "\n",
    "labeled_sentences = download_and_read(\"https://archive.ics.uci.edu/ml/machine-learning-databases/\" +\n",
    " \"00331/sentiment%20labelled%20sentences.zip\")\n",
    "sentences = [s for (s,l) in labeled_sentences]\n",
    "labels = [int(l) for (s,l) in labeled_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e24395f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 5271\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "vocab_size = len(tokenizer.word_counts)\n",
    "\n",
    "print(\"Vocabulary size: {:d}\".format(vocab_size))\n",
    "\n",
    "word2idx = tokenizer.word_index\n",
    "idx2word = {v:w for (w,v) in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f862651c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(75, 16.0), (80, 18.0), (90, 22.0), (99, 36.0), (100, 71.0)]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"An easy way to choose a good value for the maximum\n",
    "sequence length is to look at the sentence length (in number of words) at different\n",
    "percentile positions:\n",
    "\"\"\"\n",
    "\n",
    "seq_lengths = [len(s.split()) for s in sentences]\n",
    "print([(p, np.percentile(seq_lengths, p)) for p in [75, 80, 90, 99, 100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac53e4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seqlen = 64\n",
    "sentences_to_ints = tokenizer.texts_to_sequences(sentences)\n",
    "sentences_to_ints = tf.keras.preprocessing.sequence.pad_sequences(sentences_to_ints, maxlen = max_seqlen)\n",
    "labels_as_ints = np.array(labels)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((sentences_to_ints, labels_as_ints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af59868a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(10000)\n",
    "test_size = len(sentences)//3\n",
    "val_size = (len(sentences) - test_size)//10\n",
    "test_dataset = dataset.take(test_size)\n",
    "val_dataset = dataset.skip(test_size).take(val_size)\n",
    "train_dataset = dataset.skip(test_size + val_size)\n",
    "\n",
    "batch_size = 64\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "val_dataset = val_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cd0d3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64) (64,)\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_dataset.take(1):\n",
    "    print(x.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae5120bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysisModel(tf.keras.Model):\n",
    "    def __init__(self, max_seqlen, embedding_dim, vocab_sz, **kwargs):\n",
    "        super(SentimentAnalysisModel, self).__init__(**kwargs)\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            vocab_sz,\n",
    "            embedding_dim\n",
    "            )\n",
    "        self.bilstm = tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.LSTM(max_seqlen)\n",
    "        )\n",
    "        self.dense = tf.keras.layers.Dense(64, activation = 'relu')\n",
    "        self.out = tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.bilstm(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61a33735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sentiment_analysis_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  1349632   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional multiple                  164352    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  8256      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  65        \n",
      "=================================================================\n",
      "Total params: 1,522,305\n",
      "Trainable params: 1,522,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 256\n",
    "model = SentimentAnalysisModel(max_seqlen,EMBEDDING_DIM,vocab_size+1)\n",
    "model.build(input_shape = (batch_size, max_seqlen))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "131fe11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = \"binary_crossentropy\", optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68144b7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "29/29 [==============================] - 3s 107ms/step - loss: 0.5524 - accuracy: 0.7483 - val_loss: 0.3381 - val_accuracy: 0.8750\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 2s 53ms/step - loss: 0.2629 - accuracy: 0.9083 - val_loss: 0.1304 - val_accuracy: 0.9650\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 2s 52ms/step - loss: 0.1659 - accuracy: 0.9483 - val_loss: 0.1570 - val_accuracy: 0.9450\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 2s 54ms/step - loss: 0.1380 - accuracy: 0.9544 - val_loss: 0.0368 - val_accuracy: 0.9950\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 2s 55ms/step - loss: 0.0808 - accuracy: 0.9761 - val_loss: 0.0476 - val_accuracy: 0.9850\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 2s 53ms/step - loss: 0.0651 - accuracy: 0.9789 - val_loss: 0.0370 - val_accuracy: 0.9850\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 2s 53ms/step - loss: 0.0541 - accuracy: 0.9833 - val_loss: 0.0176 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 2s 54ms/step - loss: 0.0400 - accuracy: 0.9883 - val_loss: 0.0126 - val_accuracy: 0.9950\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 2s 54ms/step - loss: 0.0221 - accuracy: 0.9928 - val_loss: 0.0106 - val_accuracy: 0.9950\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 2s 54ms/step - loss: 0.0137 - accuracy: 0.9950 - val_loss: 0.0136 - val_accuracy: 0.9950\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"./data\"\n",
    "logs_dir = \"./logs\"\n",
    "best_model_file = os.path.join(data_dir, \"best_model.h5\")\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(best_model_file, save_weights_only = True,\n",
    "    save_best_only = True)\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir = logs_dir)\n",
    "num_epochs = 10\n",
    "history = model.fit(train_dataset, epochs = num_epochs, validation_data = val_dataset,\n",
    "    callbacks = [checkpoint, tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a38592c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = SentimentAnalysisModel(max_seqlen,EMBEDDING_DIM,vocab_size+1)\n",
    "best_model.build(input_shape=(batch_size, max_seqlen))\n",
    "best_model.load_weights(best_model_file)\n",
    "best_model.compile(\n",
    " loss=\"binary_crossentropy\",\n",
    " optimizer=\"adam\",\n",
    " metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98504012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 17ms/step - loss: 0.0173 - accuracy: 0.9960\n",
      "test loss: 0.017, test accuracy: 0.996\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = best_model.evaluate(test_dataset)\n",
    "print(\"test loss: {:.3f}, test accuracy: {:.3f}\".format(\n",
    " test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f59177f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LBL\tPRED\tSENT\n",
      "0\t0\toverall i was not impressed and would not go back\n",
      "1\t1\tback to good bbq lighter fare reasonable pricing and tell the public they are back to the old ways\n",
      "0\t0\tfirst of all it doesn't wear well\n",
      "0\t0\ti will not return\n",
      "0\t0\tso don't go there if you are looking for good food\n",
      "0\t0\tnot recommended\n",
      "1\t1\tthe fact is this film is a wonderful heartwarming tale about two people chasing their dreams\n",
      "1\t1\tthere still are good actors around\n",
      "1\t1\tjulian fellowes has triumphed again\n",
      "0\t0\tno one at the table thought the food was above average or worth the wait that we had for it\n",
      "0\t0\tthey refuse to refund or replace\n",
      "1\t1\treally really good rice all the time\n",
      "0\t0\tbut she is still a bad actress repeating her robotic face moves in each of her pictures\n",
      "1\t1\ti received my headset in good time and was happy with it\n"
     ]
    }
   ],
   "source": [
    "labels, predictions = [], []\n",
    "idx2word[0] = \"PAD\"\n",
    "is_first = True\n",
    "print(\"LBL\\tPRED\\tSENT\")\n",
    "for test_batch in test_dataset:\n",
    "    inputs_b, labels_b = test_batch\n",
    "    pred_batch = best_model.predict(inputs_b)\n",
    "    predictions.extend([(1 if p > 0.5 else 0) for p in pred_batch])\n",
    "    labels.extend([l for l in labels_b])\n",
    "    if is_first:\n",
    "        for i in range(inputs_b.shape[0]-50):\n",
    "            words = [idx2word[idx] for idx in inputs_b[i].numpy()]\n",
    "            words = [w for w in words if w != \"PAD\"]\n",
    "            sentences = \" \".join(words)\n",
    "            print(\"{:d}\\t{:d}\\t{:s}\".format(labels[i], predictions[i], sentences))\n",
    "        is_first = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81d52c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score: 0.998\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy score: {:.3f}\".format(accuracy_score(labels, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f1d4c659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix\n",
      "[[507   1]\n",
      " [  1 491]]\n"
     ]
    }
   ],
   "source": [
    "print(\"confusion matrix\")\n",
    "print(confusion_matrix(labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0ab31d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
